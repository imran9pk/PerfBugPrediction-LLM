{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hugging Face transformers for performance bug prediction with codellama/CodeLlama-7b-hf\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, CodeLlamaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Hugging Face Authentication\n",
    "print(\"Setting up Hugging Face authentication...\")\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"  \n",
    "\n",
    "#Set up Hugging Face model, to be used later for 1- Tokenizer, 2- Model Initialization\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "#paths used\n",
    "input_dir = 'data/input'\n",
    "src_files_dir = 'data/src_files-sampled'\n",
    "output_dir = 'data/output'\n",
    "sampled_input_csv = os.path.join(input_dir, 'sampled_files_codelama_experiment.csv')\n",
    "results_csv = os.path.join(output_dir, 'codellama_classification_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the model configuration\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Print the model's maximum token limit\n",
    "print(f\"Max token length for {model_name}: {config.max_position_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV File\n",
    "print(\"Loading CSV file...\")\n",
    "sampled_files = pd.read_csv(sampled_input_csv)\n",
    "print(\"CSV file loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open files safely, handling long paths and normalization\n",
    "def open_file(project_name, github_path):\n",
    "    # Ensure github_path is clean (remove any leading slashes)\n",
    "    github_path_clean = github_path.lstrip('/').lstrip('\\\\')\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(src_files_dir, project_name, github_path_clean)\n",
    "    file_path = os.path.normpath(file_path)\n",
    "    abs_file_path = os.path.abspath(file_path)\n",
    "    \n",
    "    # Apply the \\\\?\\ prefix for long paths on Windows\n",
    "    if os.name == 'nt' and len(abs_file_path) >= 260:\n",
    "        abs_file_path = f\"\\\\\\\\?\\\\{abs_file_path}\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(abs_file_path):\n",
    "        try:\n",
    "            with open(abs_file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {abs_file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"File not found: {abs_file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Dataset\n",
    "print(\"Preparing the dataset...\")\n",
    "\n",
    "# Prepare lists to store data\n",
    "codes = []\n",
    "labels = []\n",
    "project_names = []\n",
    "github_paths = []\n",
    "\n",
    "# Loop over all files in the CSV\n",
    "for index, file_row in sampled_files.iterrows():\n",
    "    project_name = file_row['Project_name']\n",
    "    github_path = file_row['github_path']\n",
    "    label = file_row['label']  # Assuming label is 0 or 1\n",
    "\n",
    "    # Read the Java file\n",
    "    java_code = open_file(project_name, github_path)\n",
    "    if java_code is None:\n",
    "        print(f\"Failed to read the Java file at {github_path}. Skipping.\")\n",
    "        continue  # Skip this file and continue with the next\n",
    "\n",
    "    # Append data to the lists\n",
    "    codes.append(java_code)\n",
    "    labels.append(label)\n",
    "    project_names.append(project_name)\n",
    "    github_paths.append(github_path)\n",
    "\n",
    "# Create the dataset\n",
    "data = {'code': codes, 'label': labels}\n",
    "dataset = Dataset.from_dict(data)\n",
    "print(f\"Dataset prepared successfully with {len(codes)} code snippets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Dataset and find the maximum number of tokens and frequency of tokens greater than 16,384\n",
    "print(\"Checking the maximum number of tokens in the dataset...\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to tokenize each example and return token lengths\n",
    "def get_token_lengths(examples):\n",
    "    # Tokenize without truncation and return token lengths\n",
    "    tokenized = tokenizer(examples['code'], truncation=False)\n",
    "    return {\"token_length\": [len(t) for t in tokenized[\"input_ids\"]]}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "token_lengths = dataset.map(get_token_lengths, batched=True)\n",
    "\n",
    "# Find the maximum token length\n",
    "max_tokens = max(token_lengths[\"token_length\"])\n",
    "print(f\"The maximum number of tokens in the dataset is: {max_tokens}\")\n",
    "\n",
    "# Count how many examples exceed the token limit of 16,384\n",
    "token_limit = 16384\n",
    "exceeding_tokens_count = sum(1 for length in token_lengths[\"token_length\"] if length > token_limit)\n",
    "print(f\"Number of code snippets exceeding {token_limit} tokens: {exceeding_tokens_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the Dataset\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['code'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialize the Model\n",
    "print(\"Initializing the model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, use_auth_token=os.environ[\"HF_TOKEN\"])\n",
    "print(\"Model initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Zero-Shot Classification\n",
    "print(\"Performing zero-shot classification...\")\n",
    "\n",
    "def zero_shot_classification(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions.item()\n",
    "\n",
    "print(\"Performing zero-shot classification on all code snippets...\")\n",
    "predictions = []\n",
    "for code_snippet in codes:\n",
    "    #Add some interaactivity to show progress\n",
    "    print(f\"Processing code snippet {codes.index(code_snippet)+1} of {len(codes)}\")\n",
    "    prediction = zero_shot_classification(code_snippet)\n",
    "    predictions.append(prediction)\n",
    "print(\"Zero-shot classification completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save Results (optional)\n",
    "print(\"Saving results...\")\n",
    "predictions_df = pd.DataFrame({'project_name': project_names,\n",
    "                                'github_path': github_paths,\n",
    "                                  'label': labels,\n",
    "                                    'prediction': predictions})\n",
    "predictions_df.to_csv(results_csv, index=False)\n",
    "print(predictions_df.shape)\n",
    "print(\"Results saved to codellama_classification_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(predictions_df['label'], predictions_df['prediction'])\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(\"Confusion matrix generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all the metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(predictions_df['label'], predictions_df['prediction'])\n",
    "precision = precision_score(predictions_df['label'], predictions_df['prediction'])\n",
    "recall = recall_score(predictions_df['label'], predictions_df['prediction'])\n",
    "f1 = f1_score(predictions_df['label'], predictions_df['prediction'])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"All metrics computed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codellama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process all the files in the sampled dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Hugging Face API URL for zero-shot classification\n",
    "api_url = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
    "hf_key = \"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\"\n",
    "\n",
    "# Function to handle long paths by applying the \\\\?\\ prefix for Windows if needed\n",
    "def handle_long_path(file_path):\n",
    "    abs_file_path = os.path.abspath(file_path)\n",
    "    if os.name == 'nt' and len(abs_file_path) >= 260:  # Windows specific and path length limit\n",
    "        file_path = f\"\\\\\\\\?\\\\{abs_file_path}\"\n",
    "    return file_path\n",
    "\n",
    "# Function to process file content with encoding handling\n",
    "def process_file(file_path):\n",
    "    file_path = handle_long_path(file_path)  # Handle long paths\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None, -1\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                file_content = f.read()\n",
    "            print(f\"Processed with latin-1 encoding: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            return None, -1\n",
    "\n",
    "    return file_content, 1\n",
    "\n",
    "# Function to call Hugging Face API for zero-shot classification\n",
    "def run_zero_shot_classification(file_content):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {hf_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": file_content,\n",
    "        \"parameters\": {\n",
    "            \"candidate_labels\": [\"1\", \"0\"],  # Labels for classification\n",
    "            \"multi_label\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        # Get the label with the highest score\n",
    "        predicted_label = result['labels'][0]\n",
    "        return predicted_label, None\n",
    "    else:\n",
    "        return None, f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "\n",
    "\n",
    "#paths used\n",
    "src_dir = 'data/src_files-sampled'\n",
    "input_dir = 'data/input'\n",
    "output_dir = 'data/output'\n",
    "batch_dir = 'data/output/batches'\n",
    "input_csv = os.path.join(input_dir, 'sampled_files_codelama_experiment.csv')\n",
    "combined_output_csv = os.path.join(output_dir, 'samples_codelama_api_output.csv')\n",
    "\n",
    "# Load the sampled dataset CSV\n",
    "sampled_files = pd.read_csv(input_csv)\n",
    "\n",
    "# Define batch size and the number of files per chunk\n",
    "batch_size = 25  # Number of rows per chunk\n",
    "batch_number = 0  # Start with the first batch\n",
    "\n",
    "# Get total number of rows\n",
    "total_rows = sampled_files.shape[0]\n",
    "print(f\"Total number of files: {total_rows}\")\n",
    "\n",
    "# Loop through the dataset in chunks\n",
    "for start_idx in range(0, total_rows, batch_size):\n",
    "    # Define the end index for the current batch\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch = sampled_files.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "    # Prepare lists to store the results and error messages for the current batch\n",
    "    predictions = []\n",
    "    error_messages = []\n",
    "\n",
    "    for idx, row in batch.iterrows():\n",
    "        project_name = row['Project_name']\n",
    "        github_path = row['github_path'].lstrip('/')  # Clean up the path by removing leading slashes\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(src_dir, project_name, github_path)\n",
    "        file_path = os.path.normpath(file_path)\n",
    "\n",
    "        # Process the file with encoding handling\n",
    "        file_content, status = process_file(file_path)\n",
    "\n",
    "        # Run Hugging Face API inference if the file was processed successfully\n",
    "        if status != -1:\n",
    "            try:\n",
    "                prediction, error_message = run_zero_shot_classification(file_content)\n",
    "                predictions.append(prediction)\n",
    "                error_messages.append(error_message)  # Append error message (None if no error)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during Hugging Face API inference for file {file_path}: {e}\")\n",
    "                predictions.append(None)\n",
    "                error_messages.append(str(e))\n",
    "        else:\n",
    "            predictions.append(None)\n",
    "            error_messages.append(f\"File processing failed for {file_path}\")\n",
    "\n",
    "    # Add the predictions and error messages to the batch DataFrame\n",
    "    batch['api_prediction'] = predictions\n",
    "    batch['api_error_message'] = error_messages\n",
    "\n",
    "    # Save the current batch to a separate CSV file \n",
    "    # and combine the name with the output directory\n",
    "    batch_file_name = os.path.join(batch_dir, f'batch_{batch_number}.csv')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(batch_file_name), exist_ok=True)\n",
    "    batch.to_csv(batch_file_name, index=False)\n",
    "    print(f\"Batch {batch_number} saved as {batch_file_name}\")\n",
    "\n",
    "    # Increment batch number for the next iteration\n",
    "    batch_number += 1\n",
    "\n",
    "# At the end, combine all batch CSVs into a single final CSV file\n",
    "# and join path with csv_files\n",
    "csv_files = [os.path.join(batch_dir, f'batch_{i}.csv') for i in range(batch_number)]\n",
    "\n",
    "# Load all batch CSVs and combine them into one DataFrame\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in csv_files])\n",
    "\n",
    "# Save the combined result into a final CSV file\n",
    "os.makedirs(os.path.dirname(combined_output_csv), exist_ok=True)\n",
    "combined_df.to_csv(combined_output_csv, index=False)\n",
    "print(f\"All batches combined and saved as {combined_output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the batch files in the batch directory\n",
    "for f in csv_files:\n",
    "    os.remove(f)\n",
    "    print(f\"Deleted {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
